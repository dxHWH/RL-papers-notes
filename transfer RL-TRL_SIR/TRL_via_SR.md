# Transfer RL via Scenario Independent Representation (TRL-SIR)

## 摘要

多智能体强化学习（MARL）算法被广泛应用于解决动态多智能体系统（MAS）中需要智能体间协作与竞争的复杂任务。然而，从零开始学习此类任务不仅困难重重，在交互智能体数量庞大的MAS中更可能因极高的样本复杂度而难以实现。因此，复用历史经验或其他智能体获得的知识可有效加速学习过程并提升MARL算法的扩展性。

本研究提出了一种新型框架，通过将异构状态空间统一为固定尺寸的输入，使单一深度学习策略能适用于MAS中不同场景，从而实现MARL的迁移学习。

具体来说：
本方法提出了一种新颖的空间与特征编码框架：
- 该框架通过统一智能体个体向神经网络输入的状态信息，并采用通用化输出表征，可适用于不同多智能体场景。
- 运用空间抽象技术——影响图（Influence Map, IM），结合多智能体影响图（Multi-Agent Influence Map, MAIM）提取的全局信息，**将各类局部观测统一至固定维度，使智能体获得场景无关的决策能力**。
- **定义一种融合局部观测与抽象全局状态的统一状态表征框架来实现**。此外，智能体的历史状态与当前状态信息被整合至该统一表征中，以支持多智能体训练过程中的精细化决策优化

这种空间与特征表征会与智能体历史及当前状态进行聚合，并通过固定规模的神经网络策略进行训练，从而在多智能体系统中保持跨场景的领域知识。

在《星际争霸》多智能体挑战（SMAC）环境的多种场景中评估该方法，结果表明：与从零学习的智能体相比，复用其他场景习得的机动技能能显著提升多智能体学习性能。
此外，本方法采用课程迁移学习（CTL）方法，使深度学习策略能按难度等级设计的同质化学习场景逐步获取知识与技能。该过程促进了智能体间与智能体内的知识迁移，从而在更复杂的异质化场景中实现优异的多智能体学习性能。



## 研究背景
为降低样本复杂度并加速自主智能体处理复杂学习任务的过程，迁移学习（TL）日益受到研究者关注。该技术主张通过**复用既往任务**或**外部来源（如人类示范行为或其他学习智能体的建议）**的知识来加速学习进程。**未经系统设计的知识复用可能导致负迁移现象，反而增加学习难度**。

### 单智能体迁移学习
单智能体强化学习的迁移学习技术已取得长足发展，能够适用于复杂应用场景

